{
  "name": "Jnf neat",
  "tagline": "Actually usable implementation of NEAT",
  "body": "# JNF_NEAT\r\n\r\nMy implementation of Kenneth Stanley and Risto Miikkulainen's NEAT (NeuroEvolution\r\nof Augmenting Topologies, http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf).\r\n\r\nIt focuses (in contrast to other implementations) on\r\n\r\n- Speed - through modern and efficient C++14\r\n- Clean Code - through constant ongoing refactoring and a deep care for aesthetics\r\n- Usability - through beeing able to be used without much knowledge of Neural Networks\r\n\r\n##Foreword\r\n\r\n###Motivation\r\nOne of the problems of machine learning is its steep learning curve. If you want to let your code learn,\r\nyou've got few choices: Use expensive proprietary modules or use libraries that are aimed at people with experience in AI.\r\nIf you want to learn how machine learning works, you've got to read scientific papers with tons of buzzwords.\r\nThe provided code is often made by people who are brilliant at researching, but have only got poor programming skills.\r\n\r\nWe think this is a shame. A programmer shouldn't have to learn about the internals of a class.\r\nHe should be able to use it intuitively, in a matter that it relieves him from work, not set him up for more.\r\nThat's the age old principle of encapsulation!\r\n\r\nAnother problem we've encountered, was platform dependency. Despite being conceived as a platform independent language,\r\nwe encountered various implementations of NEAT, that managed to break this. Examples included code with for loops looking like this:\r\n```sh\r\nfor( i = 0; i < 10; i++ ){\r\n    ...\r\n}\r\n```\r\nNotice the `i` not having been declared? Some compilers permit this. Another, more subtle one was the `abstract` keyword\r\nof Microsoft Visual Studio. It's actually not part of the language. The official way is to write an (admittedly more arbitrary) `= 0`\r\nat the end of a function.\r\n\r\n###What is NEAT?\r\n\r\nTODO\r\n\r\n##Usage\r\nFirst, you have to instantiate a `NeuralNetworkTrainer`. This class will take care of everything. It uses standard training values if not provided with a parameter.\r\nIf you know what you're doing, you can provide it with a `NeuralNetworkTrainer::RuleSet` instance to tweak the learning process.\r\n\r\nYou then have to provide an implementation of the `ITrainable` interface. It's methods are\r\n- Update()\r\n- GetOrCalculateFitness()\r\n- ReceiveNetworkOutputs()\r\n- ProvideNetworkWithInputs()\r\n\r\n###Update()\r\nThis method gets called automatically multiple times during training.\r\n> default updatesPerGeneration: 1\r\n> Imagine this value as **number of actions per lifetime**\r\n\r\nThe actions of your object should take place here. This almost always boils down to **executing the command the Neural Network decides to use**. (Remember: You get this Information by calling `LoadNeuralNetworkOutputs()`).\r\n\r\n**Example**: Say you want to train an artificial player for Super Mario World. This method should then take care of actually pressing the buttons your network want you to. In this specific case, it should also update the whole game for a frame, so enemies and items can react to Mario.\r\n\r\n###GetOrCalculateFitness()\r\nThis method tells the trainer how good this specific instance is compared to others.\r\nIt gets called automatically when the `ITrainable` object dies\r\n> It's used to generate this objects offspring, with a fitness score of zero or lower meaning that this individuals genes are not going to get passed on\r\n\r\n> default minFitness: -2147483646\r\n> default maxFitness: 100\r\n\r\nNote that in very analog programs such as real world simulations, true perfection should be unreachable\r\n\r\n**Example**: A simulated chess player could have a fitness method implemented like this:\r\n```sh\r\nunsigned int ChessSim::GetOrCalculateFitness() {\r\n  unsigned int fitness = 0;\r\n  for (const auto & piece : enemyKilledPieces) {\r\n    fitness += piece.GetImportance();\r\n  }\r\n  for (const auto & piece : ownKilledPieces) {\r\n    fitness -= piece.GetImportance();\r\n  }\r\n  return fitness;\r\n}\r\n```\r\n\r\n###ReceiveNetworkOutputs()\r\nThis method returns the conclusions of your neural network as a series of floats\r\n> default minNeuralCharge = 0.0;\r\n> default maxNeuralCharge = 1.0;\r\n> It is **highly** recommended to leave these values like this (see advanced FAQ for details)\r\n\r\nAlmost always you'll want to translate these values into something your program can work with and store in a member\r\n\r\n**Example**: TODO\r\n\r\n###ProvideNetworkWithInputs()\r\nThis method describes what your network \"sees\". It get's called automatically whenever your network needs updated real world knowledge.\r\nRemember that it works (just like `ReceiveNetworkOutputs()`) with a vector of doubles, so most of the time you'll want to translate your\r\ninputs by dividing them by their maximally possible values.\r\n> default minNeuralCharge = 0.0;  \r\n> default maxNeuralCharge = 1.0;  \r\n> It is **highly** recommended to leave these values like this (see advanced FAQ for details)\r\n\r\n**Example**: Let's assume you want to create a handwriting reader.\r\nYour input would be a 20 pixels wide and 20 pixels high image of a hand drawn letter.\r\nIn order to feed the neural network with this information,\r\nwe'll create a vector with 20 * 20 = 400 inputs, each one having a value between 0.0 and 1.0 that represents this pixels blackness.\r\nWe can do this in a variety of ways. One of them would be to add the RGB values of the pixel up and divide them by\r\nthe maximum possible, 255+255+255 = 765. By this method, a bright red pixel(255,0,0) would be represented in our 400 element vector by\r\nthe number 255 / 765 = 0.3333.\r\n\r\n###Full Example\r\n \r\n TODO\r\n \r\n## FAQ\r\n\r\nTODO\r\n\r\n## Advanced FAQ\r\n\r\nTODO\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}